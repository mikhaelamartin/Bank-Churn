{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "- Numerical vs. Target (Correlation)\n",
    "- Numerical vs. Numerical (Correlation)\n",
    "- Categorical vs. Target (Chi-Squared)\n",
    "- Categorical vs. Categorical (Chi-Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train_prepared.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test_prepared.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Card_Category</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "      <th>missing_marital_status</th>\n",
       "      <th>missing_education_level</th>\n",
       "      <th>missing_income_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>$120K +</td>\n",
       "      <td>Blue</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>9380.0</td>\n",
       "      <td>1.317</td>\n",
       "      <td>1592</td>\n",
       "      <td>34</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9238.0</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2522</td>\n",
       "      <td>68</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Uneducated</td>\n",
       "      <td>Married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>587</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.670</td>\n",
       "      <td>5121</td>\n",
       "      <td>80</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married</td>\n",
       "      <td>Less than $40K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2376</td>\n",
       "      <td>2717.0</td>\n",
       "      <td>0.822</td>\n",
       "      <td>2341</td>\n",
       "      <td>57</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>$40K - $60K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2232</td>\n",
       "      <td>716.0</td>\n",
       "      <td>0.858</td>\n",
       "      <td>3635</td>\n",
       "      <td>79</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age Gender  Dependent_count Education_Level Marital_Status  \\\n",
       "0            55      M                2        Graduate        Married   \n",
       "1            32      M                0        Graduate         Single   \n",
       "2            35      F                4      Uneducated        Married   \n",
       "3            40      F                2             NaN        Married   \n",
       "4            45      F                4     High School        Married   \n",
       "\n",
       "  Income_Category Card_Category  Months_on_book  Total_Relationship_Count  \\\n",
       "0         $120K +          Blue              50                         3   \n",
       "1             NaN          Blue              27                         6   \n",
       "2             NaN          Blue              25                         2   \n",
       "3  Less than $40K          Blue              36                         3   \n",
       "4     $40K - $60K          Blue              36                         5   \n",
       "\n",
       "   Months_Inactive_12_mon  ...  Total_Revolving_Bal  Avg_Open_To_Buy  \\\n",
       "0                       2  ...                 1536           9380.0   \n",
       "1                       3  ...                    0           9238.0   \n",
       "2                       3  ...                  587           2792.0   \n",
       "3                       1  ...                 2376           2717.0   \n",
       "4                       2  ...                 2232            716.0   \n",
       "\n",
       "   Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  \\\n",
       "0                 1.317             1592              34                1.000   \n",
       "1                 0.809             2522              68                0.478   \n",
       "2                 0.670             5121              80                0.702   \n",
       "3                 0.822             2341              57                0.541   \n",
       "4                 0.858             3635              79                0.580   \n",
       "\n",
       "   Avg_Utilization_Ratio  missing_marital_status  missing_education_level  \\\n",
       "0                  0.141                       0                        0   \n",
       "1                  0.000                       0                        0   \n",
       "2                  0.174                       0                        0   \n",
       "3                  0.467                       0                        1   \n",
       "4                  0.757                       0                        0   \n",
       "\n",
       "   missing_income_category  \n",
       "0                        0  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender (1), one hot encode, drop_first\n",
    "# card_category: blue or not (1), one hot encode\n",
    "# marital_status (2), one hot encode, drop_first\n",
    "# income_category (1) label\n",
    "# education_level (1) label \n",
    "\n",
    "def categorical_to_numerical(df):\n",
    "    \n",
    "    # ordinal: one hot encode\n",
    "    if 'Marital_Status' in df.columns:\n",
    "        df_dummies = pd.get_dummies(df[['Gender','Marital_Status']],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop(['Gender','Marital_Status'],axis=1,inplace=True)\n",
    "    else:\n",
    "        df_dummies = pd.get_dummies(df['Gender'],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop('Gender',axis=1,inplace=True)\n",
    "    \n",
    "    # nominal: label encode\n",
    "    if 'Card_Category' in df.columns:\n",
    "        card_mapping = {'Blue' : 0, 'Silver' : 1, 'Gold' : 2, 'Platinum' : 3}\n",
    "        df['Card_Category'] = df['Card_Category'].map(card_mapping)\n",
    "    \n",
    "    edu_mapping = {'Uneducated' : 0, 'High School' : 1, 'College' : 2, 'Graduate' : 3, 'Post-Graduate' : 4, 'Doctorate' : 5}\n",
    "    df['Education_Level'] = df['Education_Level'].map(edu_mapping)\n",
    "    \n",
    "    # label \n",
    "    inc_mapping = {'Less than $40K' : 0, '$40K - $60K' : 1, '$60K - $80K' : 2, '$80K - $120K' : 3, '$120K +' : 4}\n",
    "    df['Income_Category'] = df['Income_Category'].map(inc_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = categorical_to_numerical(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = categorical_to_numerical(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df):\n",
    "    df.drop(['Marital_Status_Single','Gender_M','Marital_Status_Married','Card_Category','Dependent_count','missing_marital_status','missing_education_level','missing_income_category','Contacts_Count_12_mon','Credit_Limit','Months_on_book','Avg_Open_To_Buy','Total_Trans_Amt','Avg_Utilization_Ratio','Total_Amt_Chng_Q4_Q1','Total_Relationship_Count'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Total_Trans_Ct`: This variable explains approximately 14% of the variation of bank attrition. Surprisingly, `Total_Trans_Amt` has a much lower percentage of variation of bank attrition explained (approximately 5%) even though both variables are highly correlated with each other. Therefore, we will keep `Total_Trans_Ct` and drop `Total_Trans_Amt`.\n",
    "\n",
    "The other variables not included in the first list do not explain a large percentage of variation in churn so we will drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2 Key Findings**\n",
    "\n",
    "`Credit_Limit`,`Avg_Open_To_Buy`,0.872472: Since Avg_Open_To_Buy is part of the equation of solving Credit_Limit, this correlation is not suprising. We will drop both features since both are not correlated highly with the target variable.\n",
    "\n",
    "`Total_Trans_Ct`,`Total_Trans_Amt`,0.771498: Since Total_Trans_Ct is more correlated with the target variable, we will keep it and drop Total_Trans_Amt.\n",
    "\n",
    "`Customer_Age`,`Months_on_book`,0.591667: We will drop Months_on_book.\n",
    "\n",
    "`Total_Revolving_Bal`,`Avg_Utilization_Ratio`,0.507283: We will just keep Total_Revolving_Bal since it is used in the equation to find Avg_Utilization_Ratio and it is more correlated with the target variable.\n",
    "\n",
    "`Avg_Open_To_Buy`,`Avg_Utilization_Ratio`,0.459306: We will  drop both features because Avg_Open_To_Buy isn't highly correlated with the target variable and Avg_Utilization_Ratio is correlated with another feature that is highly correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs= X_train.copy()\n",
    "drop_features(X_train_fs)\n",
    "X_train_fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1597</td>\n",
       "      <td>59</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>67</td>\n",
       "      <td>0.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1285</td>\n",
       "      <td>33</td>\n",
       "      <td>1.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2071</td>\n",
       "      <td>71</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Education_Level  Income_Category  Months_Inactive_12_mon  \\\n",
       "0            44              0.0              2.0                       2   \n",
       "1            44              NaN              NaN                       3   \n",
       "2            37              1.0              0.0                       1   \n",
       "3            34              3.0              1.0                       2   \n",
       "4            51              1.0              0.0                       4   \n",
       "\n",
       "   Total_Revolving_Bal  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  \n",
       "0                 1597              59                0.903  \n",
       "1                    0              60                0.538  \n",
       "2                 2216              67                0.489  \n",
       "3                 1285              33                1.200  \n",
       "4                 2071              71                0.972  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fs= X_test.copy()\n",
    "drop_features(X_test_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "We will perform normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "scaler_fs = MinMaxScaler()\n",
    "X_train_fs = pd.DataFrame(scaler_fs.fit_transform(X_train_fs.values), columns=X_train_fs.columns, index=X_train_fs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df(scaler_type, df):\n",
    "    return pd.DataFrame(scaler_type.fit_transform(df.values), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_scaled.csv',index=False)\n",
    "X_train_fs.to_csv('X_train_fs_scaled.csv',index=False)\n",
    "\n",
    "X_test_fs.to_csv('X_test_fs_scaled.csv',index=False)\n",
    "X_test.to_csv('X_test_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Metrics:\n",
    "\n",
    "We care the most about recall because as the credit card company, you are more concerned about people who are likely to attrite. False positives are people who we think are going to attrite but don't actually attrite. False negatives are people who we think are going to stay, but acutally levae. We care more about minimizing this group because they are more costly. WE want Recall for people who churn to be high. This means we want a large proportion of people who are positive are properly classified as positive and small portion classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_train_alg,y_pred_alg, alg):\n",
    "    # calculate p-r curves\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train_alg, y_pred_alg)\n",
    "    # convert to f score\n",
    "    fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    print(alg)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "    pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "    pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label=alg+' Best',zorder=10)\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train_res,y_train_res)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "imba_pipeline = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              LogisticRegression())\n",
    "new_params = {'logisticregression__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True, n_iter = 100, verbose=2, n_jobs = -1)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "# grid_imba.best_params_\n",
    "# {'logisticregression__penalty': 'l2', 'logisticregression__C': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best=LogisticRegression(C=1.0,penalty=\"l2\")\n",
    "logreg_pipeline = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              logreg_best)\n",
    "logreg_pipeline.fit(X_train,y_train)\n",
    "y_pred_logreg = logreg_pipeline.predict_proba(X_train)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_logreg,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "imba_pipeline_fs = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              LogisticRegression())\n",
    "new_params = {'logisticregression__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline_fs, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True, n_iter = 100, verbose=2, n_jobs = -1)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);\n",
    "# grid_imba.best_params_\n",
    "# {'logisticregression__penalty': 'l2', 'logisticregression__C': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best_fs=LogisticRegression(C=0.1,penalty=\"l2\")\n",
    "logreg_pipeline_fs = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              logreg_best_fs)\n",
    "logreg_pipeline_fs.fit(X_train_fs,y_train)\n",
    "y_pred_logreg_fs = logreg_pipeline_fs.predict_proba(X_train_fs)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_logreg_fs,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "loss = ['deviance','exponential']\n",
    "learning_rate = [.001,.01,.1]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50,5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [20,30,50,100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [20,30,50,100]\n",
    "# Create the random grid\n",
    "random_grid = {'loss': loss,\n",
    "               'learning_rate':learning_rate,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['friedman_mse', 'mse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              GradientBoostingClassifier())\n",
    "new_params = {'gradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "\n",
    "## grid_imba.best_params_\n",
    "\n",
    "# {'gradientboostingclassifier__n_estimators': 200,\n",
    "#  'gradientboostingclassifier__min_samples_split': 30,\n",
    "#  'gradientboostingclassifier__min_samples_leaf': 50,\n",
    "#  'gradientboostingclassifier__max_features': 'auto',\n",
    "#  'gradientboostingclassifier__max_depth': 30,\n",
    "#  'gradientboostingclassifier__loss': 'deviance',\n",
    "#  'gradientboostingclassifier__learning_rate': 0.1,\n",
    "#  'gradientboostingclassifier__criterion': 'friedman_mse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_best = GradientBoostingClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=50,max_features='auto',max_depth=30,loss='deviance',learning_rate=.1,criterion='friedman_mse')\n",
    "\n",
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              gboost_best)\n",
    "imba_pipeline.fit(X_train, y_train)\n",
    "y_pred_gboost = imba_pipeline.predict_proba(X_train)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_gboost,'GBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              GradientBoostingClassifier())\n",
    "new_params = {'gradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_best_fs = GradientBoostingClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=50,max_features='auto',max_depth=30,loss='deviance',learning_rate=.1,criterion='friedman_mse')\n",
    "\n",
    "imba_pipeline_fs = make_pipeline(SMOTE(), \n",
    "                              gboost_best_fs)\n",
    "imba_pipeline_fs.fit(X_train, y_train)\n",
    "y_pred_gboost_fs = imba_pipeline_fs.predict_proba(X_train_fs)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_gboost_fs,'GBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "base_estimator = ['none',LogisticRegression()]\n",
    "# Minimum number of samples required to split a node\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# Minimum number of samples required at each leaf node\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'base_estimator': base_estimator,\n",
    "               'learning_rate': learning_rate,\n",
    "              'algorithm': algorithm}\n",
    "\n",
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              AdaBoostClassifier())\n",
    "new_params = {'adaboostclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train,y_train)\n",
    "grid_imba.best_params_\n",
    "\n",
    "# {'n_estimators': 300,\n",
    "#  'learning_rate': 0.1,\n",
    "#  'base_estimator': LogisticRegression(),\n",
    "#  'algorithm': 'SAMME.R'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best = AdaBoostClassifier(n_estimators=300,base_estimator=LogisticRegression(),learning_rate=1.0,algorithm='SAMME')\n",
    "ada_pipeline = make_pipeline(SMOTE(), \n",
    "                              ada_best)\n",
    "ada_pipeline.fit(X_train, y_train)\n",
    "y_pred_ada = ada_best.predict_proba(X_train)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_ada,'ADA Boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              AdaBoostClassifier())\n",
    "new_params = {'adaboostclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs,y_train)\n",
    "grid_imba_fs.best_params_\n",
    "\n",
    "# {'adaboostclassifier__n_estimators': 250,\n",
    "#  'adaboostclassifier__learning_rate': 0.01,\n",
    "#  'adaboostclassifier__base_estimator': LogisticRegression(),\n",
    "#  'adaboostclassifier__algorithm': 'SAMME'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best_fs = AdaBoostClassifier(n_estimators=250,base_estimator=LogisticRegression(),learning_rate=.01,algorithm='SAMME')\n",
    "ada_pipeline_fs = make_pipeline(SMOTE(), \n",
    "                              ada_best_fs)\n",
    "ada_pipeline_fs.fit(X_train_fs, y_train)\n",
    "y_pred_ada_fs = ada_best_fs.predict_proba(X_train_fs)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_ada_fs,'ADA Boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50,5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [20,30,50,100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [20,30,50,100]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['entropy','gini']}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              RandomForestClassifier())\n",
    "new_params = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "##new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "\n",
    "# grid_imba.best_params_\n",
    "# {'randomforestclassifier__n_estimators': 200,\n",
    "#  'randomforestclassifier__min_samples_split': 30,\n",
    "#  'randomforestclassifier__min_samples_leaf': 20,\n",
    "#  'randomforestclassifier__max_features': 'log2',\n",
    "#  'randomforestclassifier__max_depth': 50,\n",
    "#  'randomforestclassifier__criterion': 'gini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=20,max_features='log2',max_depth=50,criterion='gini')\n",
    "rf_pipeline = make_pipeline(SMOTE(), rf_best)\n",
    "rf_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = rf_pipeline.predict_proba(X_train)[:,1]\n",
    "plot_precision_recall_curve(y_train,y_pred_rf,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              RandomForestClassifier())\n",
    "new_params = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "##new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=250,min_samples_split=50,min_samples_leaf=50,max_features='log2',max_depth=20,criterion='entropy')\n",
    "rf_pipeline_fs = make_pipeline(SMOTE(), rf_best)\n",
    "rf_pipeline_fs.fit(X_train_fs,y_train)\n",
    "\n",
    "y_pred_rf_fs = rf_pipeline_fs.predict_proba(X_train_fs)[:,1]\n",
    "plot_precision_recall_curve(y_train,y_pred_rf_fs,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Feature Selection Precision_Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'Logistic Regression'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_logreg)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "alg = 'Random Forest'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_rf)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'ADA Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_ada)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'Gradient Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_gboost)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Precision_Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'Logistic Regression'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_logreg_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "alg = 'Random Forest'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_rf_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'ADA Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_ada_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_curves(y_train_alg,y_pred_alg,model):\n",
    "    alg = model\n",
    "    # calculate p-r curves\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train_alg, y_pred_alg)\n",
    "    # convert to f score\n",
    "    fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    print(alg)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_train_alg[y_train_alg==1]) / len(y_train_alg)\n",
    "    pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_curves(y_train,y_pred_logreg_fs,'Logistic Regression FS')\n",
    "plt_curves(y_train,y_pred_ada_fs,'ADA Boost FS')\n",
    "plt_curves(y_train,y_pred_gboost_fs,'Gradient Boost FS')\n",
    "plt_curves(y_train,y_pred_rf_fs,'Random Forest FS')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "plt_curves(y_train,y_pred_logreg,'Logistic Regression')\n",
    "plt_curves(y_train,y_pred_ada,'ADA Boost')\n",
    "plt_curves(y_train,y_pred_gboost,'Gradient Boost')\n",
    "plt_curves(y_train,y_pred_rf,'Random Forest')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying Key Factors\n",
    "rf_best.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_curves(y_test,rf_best.predict_proba(X_test)[:,1],'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
